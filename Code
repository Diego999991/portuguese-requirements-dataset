"""
Datareq: Classification Models Demo
====================================
Implementation of SVM and Naive Bayes models for requirements classification.

This script demonstrates the three classification tasks presented in the paper:
1. Functionality Classification (Functional vs Non-Functional)
2. Single Category Classification
3. Multi-Category Classification (Top-2)

Requirements:
- pandas
- scikit-learn
- numpy
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')


class DatareqClassifier:
    """
    Classifier for software requirements using the Datareq dataset.
    """
    
    def __init__(self, data_path='datareq.csv'):
        """
        Initialize the classifier and load the dataset.
        
        Args:
            data_path (str): Path to the Datareq CSV file
        """
        self.df = pd.read_csv(data_path)
        self.vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        self.label_encoder = LabelEncoder()
        
        print(f"Dataset loaded: {len(self.df)} requirements")
        print(f"Columns: {list(self.df.columns)}")
    
    def preprocess_data(self, text_column='description'):
        """
        Preprocess and vectorize text data.
        
        Args:
            text_column (str): Name of the column containing requirement descriptions
        """
        self.X = self.vectorizer.fit_transform(self.df[text_column])
        print(f"Text vectorized: {self.X.shape[1]} features")
    
    def functionality_classification(self):
        """
        Task 1: Classify requirements as Functional or Non-Functional.
        
        Returns:
            dict: Results for SVM and Naive Bayes models
        """
        print("\n" + "="*60)
        print("TASK 1: FUNCTIONALITY CLASSIFICATION")
        print("="*60)
        
        y = self.df['functionality']  # Assumes 'functionality' column exists
        X_train, X_test, y_train, y_test = train_test_split(
            self.X, y, test_size=0.25, random_state=42, stratify=y
        )
        
        results = {}
        
        # SVM with linear kernel (C=3 as per paper)
        print("\n--- SVM (Linear Kernel, C=3) ---")
        svm_model = SVC(kernel='linear', C=3, random_state=42)
        svm_model.fit(X_train, y_train)
        y_pred = svm_model.predict(X_test)
        
        svm_acc = accuracy_score(y_test, y_pred)
        svm_f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"Accuracy: {svm_acc:.2%}")
        print(f"F1-Score: {svm_f1:.2%}")
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))
        
        results['svm'] = {'accuracy': svm_acc, 'f1_score': svm_f1}
        
        # Naive Bayes (Multinomial)
        print("\n--- Naive Bayes (Multinomial) ---")
        nb_model = MultinomialNB()
        nb_model.fit(X_train, y_train)
        y_pred = nb_model.predict(X_test)
        
        nb_acc = accuracy_score(y_test, y_pred)
        nb_f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"Accuracy: {nb_acc:.2%}")
        print(f"F1-Score: {nb_f1:.2%}")
        
        results['naive_bayes'] = {'accuracy': nb_acc, 'f1_score': nb_f1}
        
        return results
    
    def category_classification(self, category_column='category_1'):
        """
        Task 2: Classify requirements into one of 13 categories.
        Uses functionality as an additional feature.
        
        Args:
            category_column (str): Name of the primary category column
            
        Returns:
            dict: Results for SVM and Naive Bayes models
        """
        print("\n" + "="*60)
        print("TASK 2: SINGLE CATEGORY CLASSIFICATION")
        print("="*60)
        
        # Add functionality as a feature
        X_with_func = np.hstack([
            self.X.toarray(),
            self.df['functionality'].values.reshape(-1, 1)
        ])
        
        y = self.df[category_column]
        X_train, X_test, y_train, y_test = train_test_split(
            X_with_func, y, test_size=0.25, random_state=42, stratify=y
        )
        
        results = {}
        
        # SVM with linear kernel
        print("\n--- SVM (Linear Kernel, C=3) ---")
        svm_model = SVC(kernel='linear', C=3, random_state=42)
        svm_model.fit(X_train, y_train)
        y_pred = svm_model.predict(X_test)
        
        svm_acc = accuracy_score(y_test, y_pred)
        svm_f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"Accuracy: {svm_acc:.2%}")
        print(f"F1-Score: {svm_f1:.2%}")
        print(f"\nCategory Distribution:")
        print(pd.Series(y_test).value_counts())
        
        # Confusion Matrix
        print("\nConfusion Matrix:")
        cm = confusion_matrix(y_test, y_pred)
        print(cm)
        
        results['svm'] = {'accuracy': svm_acc, 'f1_score': svm_f1}
        
        # Naive Bayes (Multinomial)
        print("\n--- Naive Bayes (Multinomial) ---")
        nb_model = MultinomialNB()
        nb_model.fit(X_train, y_train)
        y_pred = nb_model.predict(X_test)
        
        nb_acc = accuracy_score(y_test, y_pred)
        nb_f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"Accuracy: {nb_acc:.2%}")
        print(f"F1-Score: {nb_f1:.2%}")
        
        results['naive_bayes'] = {'accuracy': nb_acc, 'f1_score': nb_f1}
        
        return results
    
    def multi_category_classification_top2(self):
        """
        Task 3: Multi-Category Classification (Top-2).
        Predicts the two most likely categories for each requirement.
        
        Returns:
            dict: Results for SVM and Naive Bayes models
        """
        print("\n" + "="*60)
        print("TASK 3: MULTI-CATEGORY CLASSIFICATION (TOP-2)")
        print("="*60)
        
        # Assumes 'category_1' and 'category_2' columns exist
        y1 = self.df['category_1']
        y2 = self.df['category_2']
        
        X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(
            self.X, y1, y2, test_size=0.25, random_state=42
        )
        
        results = {}
        
        # SVM with probability estimates
        print("\n--- SVM (Linear Kernel with probability) ---")
        svm_model = SVC(kernel='linear', C=3, probability=True, random_state=42)
        svm_model.fit(X_train, y1_train)
        
        # Get probability predictions for top-2
        probas = svm_model.predict_proba(X_test)
        top2_indices = np.argsort(probas, axis=1)[:, -2:]
        classes = svm_model.classes_
        
        # Calculate Top-2 accuracy
        correct = 0
        for i, (pred_indices, true1, true2) in enumerate(zip(top2_indices, y1_test, y2_test)):
            predicted_classes = [classes[idx] for idx in pred_indices]
            if true1 in predicted_classes or true2 in predicted_classes:
                correct += 1
        
        svm_acc = correct / len(y1_test)
        print(f"Top-2 Accuracy: {svm_acc:.2%}")
        
        results['svm'] = {'accuracy': svm_acc}
        
        # Naive Bayes
        print("\n--- Naive Bayes (Multinomial) ---")
        nb_model = MultinomialNB()
        nb_model.fit(X_train, y1_train)
        
        probas = nb_model.predict_proba(X_test)
        top2_indices = np.argsort(probas, axis=1)[:, -2:]
        classes = nb_model.classes_
        
        correct = 0
        for i, (pred_indices, true1, true2) in enumerate(zip(top2_indices, y1_test, y2_test)):
            predicted_classes = [classes[idx] for idx in pred_indices]
            if true1 in predicted_classes or true2 in predicted_classes:
                correct += 1
        
        nb_acc = correct / len(y1_test)
        print(f"Top-2 Accuracy: {nb_acc:.2%}")
        
        results['naive_bayes'] = {'accuracy': nb_acc}
        
        return results
    
    def class_imbalance_mitigation(self, category_column='category_1'):
        """
        Demonstrates class imbalance mitigation using class weights.
        As discussed in Section 4.5 of the paper.
        
        Args:
            category_column (str): Name of the primary category column
            
        Returns:
            dict: Results with balanced class weights
        """
        print("\n" + "="*60)
        print("CLASS IMBALANCE MITIGATION (Balanced Weights)")
        print("="*60)
        
        y = self.df[category_column]
        X_train, X_test, y_train, y_test = train_test_split(
            self.X, y, test_size=0.25, random_state=42, stratify=y
        )
        
        # SVM with balanced class weights
        print("\n--- SVM with Balanced Class Weights ---")
        svm_balanced = SVC(kernel='linear', C=3, class_weight='balanced', random_state=42)
        svm_balanced.fit(X_train, y_train)
        y_pred = svm_balanced.predict(X_test)
        
        acc = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"Accuracy: {acc:.2%}")
        print(f"F1-Score: {f1:.2%}")
        print("\nThis approach increases the importance of minority classes.")
        
        return {'accuracy': acc, 'f1_score': f1}


def main():
    """
    Main function to run all classification tasks.
    """
    print("Datareq Classification Demo")
    print("="*60)
    
    # Initialize classifier
    # NOTE: Adjust column names according to your actual CSV structure
    classifier = DatareqClassifier('datareq.csv')
    
    # Preprocess data
    classifier.preprocess_data(text_column='description')
    
    # Run all three classification tasks
    results_func = classifier.functionality_classification()
    results_cat = classifier.category_classification()
    results_top2 = classifier.multi_category_classification_top2()
    
    # Demonstrate class imbalance mitigation
    results_balanced = classifier.class_imbalance_mitigation()
    
    # Summary
    print("\n" + "="*60)
    print("SUMMARY OF RESULTS")
    print("="*60)
    print("\nTask 1 - Functionality Classification:")
    print(f"  SVM: {results_func['svm']['accuracy']:.1%} accuracy")
    print(f"  Naive Bayes: {results_func['naive_bayes']['accuracy']:.1%} accuracy")
    
    print("\nTask 2 - Category Classification:")
    print(f"  SVM: {results_cat['svm']['accuracy']:.1%} accuracy")
    print(f"  Naive Bayes: {results_cat['naive_bayes']['accuracy']:.1%} accuracy")
    
    print("\nTask 3 - Multi-Category (Top-2):")
    print(f"  SVM: {results_top2['svm']['accuracy']:.1%} accuracy")
    print(f"  Naive Bayes: {results_top2['naive_bayes']['accuracy']:.1%} accuracy")
    
    print("\nClass Imbalance Mitigation:")
    print(f"  SVM (balanced): {results_balanced['accuracy']:.1%} accuracy")


if __name__ == "__main__":
    main()
